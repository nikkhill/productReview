{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training Data\n",
    "####To be run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare training data\n",
    "import collections, itertools\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import io\n",
    "import re\n",
    "\n",
    "def cleandata(mtext):\n",
    "    mtext = re.sub(' +', ' ', mtext)\n",
    "    mtext = re.sub('\\n+', ' ', mtext)\n",
    "    #mtext = re.sub('[0-9]+. ', '', mtext)\n",
    "    mtext = re.sub(' -', '.', mtext)\n",
    "    mtext = re.sub('\\.+', '. ', mtext)\n",
    "    mtext = mtext.strip().lower()\n",
    "    return mtext\n",
    "    \n",
    "trainprosraw = io.open(\"trainpwS.txt\", \"r\", encoding='utf-8')\n",
    "trainconsraw = io.open(\"traincwS.txt\", \"r\", encoding='utf-8')\n",
    "\n",
    "trainpros = []\n",
    "traincons = []\n",
    "for line in trainprosraw:\n",
    "    trainpros.append(line[:-1])\n",
    "for line in trainconsraw:\n",
    "    traincons.append(line[:-1])\n",
    "\n",
    "\n",
    "#trainpros = nltk.sent_tokenize(trainprosraw.read())\n",
    "#traincons = nltk.sent_tokenize(trainconsraw.read())\n",
    "#print trainpros\n",
    "prowords = []\n",
    "conwords = []\n",
    "for sent in trainpros:\n",
    "    prowords.extend(nltk.word_tokenize(sent))\n",
    "for sent in traincons:\n",
    "    conwords.extend(nltk.word_tokenize(sent))\n",
    "\n",
    "trainprosraw.close()\n",
    "trainconsraw.close()\n",
    "\n",
    "stpwrds = stopwords.words()\n",
    "#len(traincons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Classifier\n",
    "####To be run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating single word features\n",
      "evaluating best word features\n",
      "evaluating best words + bigram chi_sq word features\n",
      "accuracy: 0.988700564972\n",
      "pos precision: 0.99\n",
      "pos recall: 0.99\n",
      "neg precision: 0.987012987013\n",
      "neg recall: 0.987012987013\n",
      "Most Informative Features\n",
      "               sometimes = True              neg : pos    =     13.4 : 1.0\n",
      "                    gets = True              neg : pos    =     10.8 : 1.0\n",
      "                 problem = True              neg : pos    =      9.9 : 1.0\n",
      "                  system = True              neg : pos    =      8.2 : 1.0\n",
      "                      ok = True              neg : pos    =      8.2 : 1.0\n",
      "                 awesome = True              pos : neg    =      7.4 : 1.0\n",
      "                    know = True              neg : pos    =      7.4 : 1.0\n",
      "                     bad = True              neg : pos    =      7.4 : 1.0\n",
      "                  update = True              neg : pos    =      7.4 : 1.0\n",
      "                 display = True              pos : neg    =      6.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(featx):\n",
    "    #negids = movie_reviews.fileids('neg')\n",
    "    #posids = movie_reviews.fileids('pos')\n",
    "    \n",
    "    ##For Movie Review train:\n",
    "    #negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "    #posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "     \n",
    "    ##For product reviews train:\n",
    "    negfeats = [(featx([wrd for wrd in nltk.word_tokenize(con) if wrd not in stpwrds]), 'neg') for con in traincons]\n",
    "    posfeats = [(featx([wrd for wrd in nltk.word_tokenize(pro) if wrd not in stpwrds]), 'pos') for pro in trainpros]\n",
    "    \n",
    "    negcutoff = len(negfeats)*3/4\n",
    "    poscutoff = len(posfeats)*3/4\n",
    " \n",
    "    trainfeats = negfeats[:] + posfeats[:]\n",
    "    #trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    " \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    " \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    " \n",
    "    print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    print 'pos precision:', nltk.metrics.precision(refsets['pos'], testsets['pos'])\n",
    "    print 'pos recall:', nltk.metrics.recall(refsets['pos'], testsets['pos'])\n",
    "    print 'neg precision:', nltk.metrics.precision(refsets['neg'], testsets['neg'])\n",
    "    print 'neg recall:', nltk.metrics.recall(refsets['neg'], testsets['neg'])\n",
    "    classifier.show_most_informative_features()\n",
    "    return classifier\n",
    "    \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "print 'evaluating single word features'\n",
    "#evaluate_classifier(word_feats)\n",
    " \n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    " \n",
    "for word in prowords:\n",
    "    word_fd[word.lower()]+=1\n",
    "    label_word_fd['pos'][word.lower()]+=1\n",
    "for word in conwords:\n",
    "    word_fd[word.lower()]+=1\n",
    "    label_word_fd['neg'][word.lower()]+=1\n",
    "\n",
    "# n_ii = label_word_fd[label][word]\n",
    "# n_ix = word_fd[word]\n",
    "# n_xi = label_word_fd[label].N()\n",
    "# n_xx = label_word_fd.N()\n",
    " \n",
    "pos_word_count = label_word_fd['pos'].N()\n",
    "neg_word_count = label_word_fd['neg'].N()\n",
    "total_word_count = pos_word_count + neg_word_count\n",
    " \n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.iteritems():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "        (freq, pos_word_count), total_word_count)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "        (freq, neg_word_count), total_word_count)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "\n",
    "best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])\n",
    " \n",
    "def best_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    " \n",
    "print 'evaluating best word features'\n",
    "#mc=evaluate_classifier(best_word_feats)\n",
    " \n",
    "def best_bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    d = dict([(bigram, True) for bigram in bigrams])\n",
    "    d.update(best_word_feats(words))\n",
    "    return d\n",
    " \n",
    "print 'evaluating best words + bigram chi_sq word features'\n",
    "mc = evaluate_classifier(best_bigram_word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for doing the classification\n",
    "## We use the classifier we created earlier\n",
    "####To be run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentanalysis(text):\n",
    "    result = {}\n",
    "    complete = []\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    d = {}\n",
    "    for word in tokens:\n",
    "        d[word] = True\n",
    "    compdist = mc.prob_classify(d)\n",
    "\n",
    "    for label in compdist.samples():\n",
    "        #print(\"%s: %f\" % (label, compdist.prob(label)))\n",
    "        complete.append(compdist.prob(label))\n",
    "\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "        result[sent] = []\n",
    "        #print (sent)\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        #tokens = [t for t in tokens if t not in stpwrds]\n",
    "        d = best_bigram_word_feats(tokens)\n",
    "        #d = {}\n",
    "        #for word in tokens:\n",
    "            #d[word] = True\n",
    "        dist = mc.prob_classify(d)\n",
    "        #for label in dist.samples():\n",
    "            #print(\"%s: %f\" % (label, dist.prob(label)))\n",
    "        result[sent].append(dist.prob('pos'))\n",
    "        result[sent].append(dist.prob('neg'))\n",
    "\n",
    "    return (complete, result)\n",
    "\n",
    "def findByF(f, items):\n",
    "    for i in items:\n",
    "        flist = f.split('/')\n",
    "        for fi in flist:\n",
    "            if fi.lower().strip() in i[0]:\n",
    "                print ('\\t\\t> '.encode('utf-8')+i[0].encode('utf-8'))\n",
    "    \n",
    "def thefunction(mtext):\n",
    "    comp, res = sentanalysis(mtext)\n",
    "    for i in res:\n",
    "        res[i] = res[i][0]-res[i][1]\n",
    "    items = sorted(res.items(), key = lambda i: -abs(i[1]))\n",
    "    features = ['Cpu/Processor', 'Screen/Display', 'Battery', 'Camera', 'RAM/Memory']\n",
    "    pros, cons = assignPC(items,0.1)\n",
    "    printByFeatures(features, pros, cons)\n",
    "\n",
    "def assignPC(items, sensitivity):\n",
    "    pros=[]\n",
    "    cons=[]\n",
    "    for i in items:\n",
    "        if i[1]>sensitivity:\n",
    "            pros.append(i)\n",
    "        elif i[1]<(-1*sensitivity):\n",
    "            cons.append(i)\n",
    "    return pros,cons\n",
    "\n",
    "def printAll(items):\n",
    "    for i in items:\n",
    "        print ('\\t\\t> '.encode('utf-8')+i[0].encode('utf-8'))\n",
    "\n",
    "def printByFeatures(features, pros, cons):\n",
    "    for f in features:\n",
    "        print(f)\n",
    "        print(\"\\tPROS:\")\n",
    "        findByF(f, pros)\n",
    "        print(\"\\tCONS:\")\n",
    "        findByF(f, cons)\n",
    "    print(\"\\n\\tALLPROS:\")\n",
    "    printAll(pros)\n",
    "    print(\"\\tALLCONS:\")\n",
    "    printAll(cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting input from file\n",
    "###Run this whenever data is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mtextfile = io.open(\"reviewIP.txt\", \"r\", encoding='utf-8')\n",
    "mtext = mtextfile.read()\n",
    "mtextfile.close()\n",
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "matches = tool.check(mtext)\n",
    "\n",
    "\n",
    "mtext = cleandata(mtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language errors: 59\n",
      "Cpu/Processor\n",
      "\tPROS:\n",
      "\tCONS:\n",
      "\t\t> cpu heats a slight while using continous 3g(i guess it's the issue with many phones).\n",
      "Screen/Display\n",
      "\tPROS:\n",
      "\t\t> there are some very good features: one hand operation mode in which the display becomes suitable for using us with one hand.\n",
      "\t\t> display: display is brilliant.\n",
      "\tCONS:\n",
      "Battery\n",
      "\tPROS:\n",
      "\tCONS:\n",
      "Camera\n",
      "\tPROS:\n",
      "\t\t> camera:it is good but not excellent.\n",
      "\tCONS:\n",
      "\t\t> the phones in this range with 13mp camera are giving a grainy,non-superfine image.\n",
      "\t\t> overall it surpasses the yureka and redmi's camera.\n",
      "\t\t> camera is satisfactory or you can say above average.\n",
      "RAM/Memory\n",
      "\tPROS:\n",
      "\tCONS:\n",
      "\t\t> cons:i found the ram usage a bit high when the phone is idle.\n",
      "\n",
      "\tALLPROS:\n",
      "\t\t> themes,scrolling effects,and the quick settings,gestures(double tap one is awesome).\n",
      "\t\t> which feels great and stylish.\n",
      "\t\t> there are some very good features: one hand operation mode in which the display becomes suitable for using us with one hand.\n",
      "\t\t> camera:it is good but not excellent.\n",
      "\t\t> display: display is brilliant.\n",
      "\t\t> so overall it was a good experience on zen ui also.\n",
      "\t\t> while taking selfies.\n",
      "\t\t> in this price range.\n",
      "\t\t> reading mode,vivid modes,floating calculator etc are some of the features i overcame.\n",
      "\t\t> in this price range no doubt for that.\n",
      "\t\t> ﻿performance: awesome,no lags and a slight bugs i found in the zen ui.\n",
      "\t\t> its the best.\n",
      "\t\t> very handy.\n",
      "\t\t> and i thought this model comes with a fast charger but it doesn't.\n",
      "\t\t> !\n",
      "\t\t> it has great filters and modes:gif animation,time rewind,smart remove etc.\n",
      "\t\t> it crosses 55%.\n",
      "\tALLCONS:\n",
      "\t\t> cons:i found the ram usage a bit high when the phone is idle.\n",
      "\t\t> cpu heats a slight while using continous 3g(i guess it's the issue with many phones).\n",
      "\t\t> which can be fixed in small ota update.\n",
      "\t\t> but less heating compared to yureka.\n",
      "\t\t> it is just a normal brushed aluminium finish on a plastic case.\n",
      "\t\t> zen ui: i have used cyanogen os and miui, i think zen ui also has customiztion features like them.\n",
      "\t\t> i have set the double tap technique to overcome this issue.\n",
      "\t\t> the phones in this range with 13mp camera are giving a grainy,non-superfine image.\n",
      "\t\t> sometimes.\n",
      "\t\t> so can't tell about that) build:quality is best.\n",
      "\t\t> when we hold in our hand the loudspeaker gets covered.\n",
      "\t\t> (i haven't tested in sunlight) loudspeaker: i found it loud.\n",
      "\t\t> which we wont find in this budget.\n",
      "\t\t> overall it surpasses the yureka and redmi's camera.\n",
      "\t\t> i think that can be ignored.\n",
      "\t\t> even left hand person can use this feature.\n",
      "\t\t> camera is satisfactory or you can say above average.\n",
      "\t\t> and muting the alarm,calls.\n",
      "\t\t> and the lock button placement is not at all handy.\n",
      "\t\t> so it was not a good placement for the loudspeaker volume rocker:it's an awesome placement of button.\n",
      "\t\t> (i haven't tested it in low light either.\n"
     ]
    }
   ],
   "source": [
    "#print (mtext)\n",
    "print \"Language errors:\", len(matches)\n",
    "thefunction(mtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
